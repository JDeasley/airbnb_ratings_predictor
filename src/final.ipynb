{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data setup and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS - DATA CLEANING\n",
    "\n",
    "import re\n",
    "HMTL_TAG = re.compile('<.*?>') \n",
    "WHITESPACE = re.compile(r'\\s+')\n",
    "\n",
    "def replace_html_tags(raw_html, replace_with = ''):\n",
    "  clean_text = re.sub(HMTL_TAG, replace_with, str(raw_html))\n",
    "  return clean_text\n",
    "\n",
    "def replace_whitespace(raw_text, replace_with = ' '):\n",
    "  clean_text = re.sub(WHITESPACE, replace_with, str(raw_text))\n",
    "  return clean_text\n",
    "\n",
    "def clean_text(raw_text):\n",
    "  clean_text = replace_html_tags(raw_text)\n",
    "  clean_text = replace_whitespace(clean_text)\n",
    "  return clean_text\n",
    "\n",
    "# Clean text for every row in a pandas dataframe column. Returns new dataframe with cleaned text.\n",
    "def clean_text_batch(df: pd.DataFrame, col: int) -> pd.DataFrame:\n",
    "  df.iloc[:, col] = df.iloc[:, col].apply(lambda x: clean_text(x))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS - DATA ANALYSIS / INTERPRETATION\n",
    "\n",
    "LISTING_FIELDS = ['id','listing_url','scrape_id','last_scraped','source','name','description','neighborhood_overview','picture_url','host_id','host_url','host_name','host_since','host_location','host_about','host_response_time','host_response_rate','host_acceptance_rate','host_is_superhost','host_thumbnail_url','host_picture_url','host_neighbourhood','host_listings_count','host_total_listings_count','host_verifications','host_has_profile_pic','host_identity_verified','neighbourhood','neighbourhood_cleansed','neighbourhood_group_cleansed','latitude','longitude','property_type','room_type','accommodates','bathrooms','bathrooms_text','bedrooms','beds','amenities','price','minimum_nights','maximum_nights','minimum_minimum_nights','maximum_minimum_nights','minimum_maximum_nights','maximum_maximum_nights','minimum_nights_avg_ntm','maximum_nights_avg_ntm','calendar_updated','has_availability','availability_30','availability_60','availability_90','availability_365','calendar_last_scraped','number_of_reviews','number_of_reviews_ltm','number_of_reviews_l30d','first_review','last_review','review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value','license','instant_bookable','calculated_host_listings_count','calculated_host_listings_count_entire_homes','calculated_host_listings_count_private_rooms','calculated_host_listings_count_shared_rooms','reviews_per_month']\n",
    "REVIEW_FIELDS = ['listing_id','id','date','reviewer_id','reviewer_name','comments']\n",
    "\n",
    "# function to quickly return index of a given string in LISTING_FIELDS\n",
    "def index_in_listings(field: str) -> int:\n",
    "    return LISTING_FIELDS.index(field)\n",
    "\n",
    "# function to quickly return index of a given string in REVIEW_FIELDS\n",
    "def index_in_reviews(field: str) -> int:\n",
    "    return REVIEW_FIELDS.index(field)\n",
    "\n",
    "# function to map a string reading 't' or 'f' to a boolean\n",
    "def as_bool(t_f: str) -> bool:\n",
    "    if t_f == 't':\n",
    "        return True\n",
    "    elif t_f == 'f':\n",
    "        return False\n",
    "    else:\n",
    "        raise ValueError('Invalid value for boolean: ' + t_f)\n",
    "\n",
    "# function to normalise a numpy array to floats between 0 and 1\n",
    "def normalise(arr: np.ndarray) -> np.ndarray:\n",
    "    return arr / arr.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN LISTINGS TEXT AND SAVE TO NEW CSV\n",
    "df_listings = pd.read_csv(\"../data/listings.csv\")\n",
    "\n",
    "# print(df_listings.applymap(lambda x: isinstance(x, str)).all(0))\n",
    "\n",
    "# Names and indices of columns to clean\n",
    "COLS_TO_CLEAN = {\n",
    "    'description': 6,\n",
    "    'neighborhood_overview': 7,\n",
    "    'host_about': 14\n",
    "}\n",
    "\n",
    "for col in COLS_TO_CLEAN:\n",
    "    df_listings = clean_text_batch(df_listings, COLS_TO_CLEAN[col])\n",
    "\n",
    "# df_listings.to_csv('../data/listings_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT ONLY TARGET COLUMNS FROM LISTINGS -> SAVE TO NEW CSV\n",
    "\n",
    "df_listings = pd.read_csv(\"../data/listings.csv\")\n",
    "\n",
    "# col names: review_scores_rating,review_scores_accuracy,review_scores_cleanliness,review_scores_checkin,review_scores_communication,review_scores_location,review_scores_value\n",
    "LISTING_COLS_TO_RETAIN = {\n",
    "    'id': index_in_listings('id'),\n",
    "    'review_scores_rating': index_in_listings('review_scores_rating'),\n",
    "    'review_scores_accuracy': index_in_listings('review_scores_accuracy'),\n",
    "    'review_scores_cleanliness': index_in_listings('review_scores_cleanliness'),\n",
    "    'review_scores_checkin': index_in_listings('review_scores_checkin'),\n",
    "    'review_scores_communication': index_in_listings('review_scores_communication'),\n",
    "    'review_scores_location': index_in_listings('review_scores_location'),\n",
    "    'review_scores_value': index_in_listings('review_scores_value')\n",
    "}\n",
    "\n",
    "df_listings = df_listings.iloc[:, list(LISTING_COLS_TO_RETAIN.values())]\n",
    "df_listings.to_csv('../data/listings_targets.csv', index=False)\n",
    "\n",
    "# NOTE: Some listings do not have some or all of the review scores. We will need to account for this when we train our model.\n",
    "\n",
    "# print(df_listings.applymap(lambda x: isinstance(x, str)).all(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN REVIEWS TEXT AND SELECT ONLY KEY COLUMNS -> SAVE TO NEW CSV\n",
    "\n",
    "df_reviews = pd.read_csv(\"../data/reviews.csv\")\n",
    "df_reviews = clean_text_batch(df_reviews, 5)\n",
    "\n",
    "REVIEW_COLS_TO_RETAIN = {\n",
    "    'listing_id': index_in_reviews('listing_id'),\n",
    "    'comments': index_in_reviews('comments')\n",
    "}\n",
    "\n",
    "df_reviews = df_reviews.iloc[:, list(REVIEW_COLS_TO_RETAIN.values())]\n",
    "\n",
    "# Sort by listing_id, so reviews for a given listing are grouped together\n",
    "df_reviews = df_reviews.sort_values(by=['listing_id'])\n",
    "df_reviews.to_csv('../data/reviews_skeleton.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(243183,)\n",
      "We enjoyed our stay very much. The room was comfortable, neat and clean. There were no problems at all and the host family was very helpful and caring. They helped us planning trips or recommended sights. The house is situated in a calm neighbourhood close the the Luas and different bus lines. There are no negative aspects to mention, it was a very satisfying stay. I would recommend it and stay there again whenever I am in Dublin. \n"
     ]
    }
   ],
   "source": [
    "comments = df_reviews.iloc[:, 5].values\n",
    "print(comments.shape)\n",
    "print(comments[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\james\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\james\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2214\n",
      "(243183, 2214)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# df_max - exclude words which appear in too many documents\n",
    "# df_min - exclude words which appear in too few documents\n",
    "# Use cross validation to determine best values for these parameters.\n",
    "# HAVE TO USE MIN_DF, otherwise feature vectors are:\n",
    "# a) too large for my computer to handle.\n",
    "# b) too full of zeros to be useful.\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.2, min_df=0.001) # 93,193 features\n",
    "\n",
    "# A single doc will be a list of sentences, in this case a single review.\n",
    "# Each sentence will be a list of words, or in this case tokens, which have been stemmed.\n",
    "vectors = vectorizer.fit_transform(comments)\n",
    "print(len(vectorizer.get_feature_names_out()))\n",
    "print(vectors.shape)\n",
    "print(type(vectors))\n",
    "print(type(vectors[0]))\n",
    "print(type(vectors[0][0]))\n",
    "# print(len(vectors.toarray()))\n",
    "\n",
    "# norm_vectors = []\n",
    "# for vector in vectors.toarray():\n",
    "#     norm_vectors.append(normalise(vector))\n",
    "# norm_vectors = np.array(norm_vectors)\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import PorterStemmer\n",
    "# stemmer = PorterStemmer()\n",
    "# tokens = word_tokenize(\"Here's example text, isn't it?\")\n",
    "# stems = [stemmer.stem(token) for token in tokens]\n",
    "# print(stems)\n",
    "\n",
    "# tokens = word_tokenize(\"likes liking liked\")\n",
    "# stems = [stemmer.stem(token) for token in tokens]\n",
    "# print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1611)\t0.10034119226853999\n",
      "  (0, 1252)\t0.2606826520004324\n",
      "  (0, 1326)\t0.2716879035414134\n",
      "  (0, 1148)\t0.2735265929660197\n",
      "  (0, 292)\t0.12506550921867488\n",
      "  (0, 569)\t0.2325298183156135\n",
      "  (0, 1197)\t0.19759874300173133\n",
      "  (0, 399)\t0.11856310154980176\n",
      "  (0, 1329)\t0.2040305194885017\n",
      "  (0, 310)\t0.2364094149157616\n",
      "  (0, 1779)\t0.2093844068652601\n",
      "  (0, 986)\t0.11722260700044745\n",
      "  (0, 1762)\t0.21706501481789367\n",
      "  (0, 1615)\t0.16504067658028237\n",
      "  (0, 2031)\t0.2510637012729492\n",
      "  (0, 1490)\t0.2518556702440787\n",
      "  (0, 946)\t0.1919402492295867\n",
      "  (0, 322)\t0.24929458730293394\n",
      "  (0, 947)\t0.11926036963913877\n",
      "  (0, 757)\t0.1610923727814727\n",
      "  (0, 1539)\t0.23053478573094208\n",
      "  (0, 1318)\t0.23720957484615215\n",
      "  (0, 416)\t0.10996789961582039\n",
      "  (0, 1675)\t0.10958327466086534\n",
      "  (0, 681)\t0.14809332862871688\n"
     ]
    }
   ],
   "source": [
    "print(vectors[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to predict the ratings of a listing based on reviews for that listing.\n",
    "\n",
    "So in order to train the model, we need to group all reviews for a particular listing with the appropriate listing id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE WORKFLOW ONLY\n",
    "\n",
    "# Split data into training and test sets\n",
    "# [Maybe use k-fold to do all of this in a loop?]\n",
    "\n",
    "# 1) Train model for each target column\n",
    "# Model 1: review_scores_rating\n",
    "model1.fit(vectors[train], listings_targets[train][review_scores_rating_column])\n",
    "# Model 2: review_scores_accuracy\n",
    "model2.fit(vectors[train], listings_targets[train][review_scores_accuracy_column])\n",
    "# Model 3: review_scores_cleanliness\n",
    "model3.fit(vectors[train], listings_targets[train][review_scores_cleanliness_column])\n",
    "# Model 4: review_scores_checkin\n",
    "model4.fit(vectors[train], listings_targets[train][review_scores_checkin_column])\n",
    "# Model 5: review_scores_communication\n",
    "model5.fit(vectors[train], listings_targets[train][review_scores_communication_column])\n",
    "# Model 6: review_scores_location\n",
    "model6.fit(vectors[train], listings_targets[train][review_scores_location_column])\n",
    "# Model 7: review_scores_value\n",
    "model7.fit(vectors[train], listings_targets[train][review_scores_value_column])\n",
    "\n",
    "# 2) Predict for each target column\n",
    "# Model 1: review_scores_rating\n",
    "model1.predict(vectors[test])\n",
    "# Model 2: review_scores_accuracy\n",
    "model2.predict(vectors[test])\n",
    "# Model 3: review_scores_cleanliness\n",
    "model3.predict(vectors[test])\n",
    "# Model 4: review_scores_checkin\n",
    "model4.predict(vectors[test])\n",
    "# Model 5: review_scores_communication\n",
    "model5.predict(vectors[test])\n",
    "# Model 6: review_scores_location\n",
    "model6.predict(vectors[test])\n",
    "# Model 7: review_scores_value\n",
    "model7.predict(vectors[test])\n",
    "\n",
    "# 3) Evaluate each model\n",
    "# NOTE: [I don't know how to do this yet. Copilot wrote this for me. Might be where we do k-fold cross validation.]\n",
    "# Model 1: review_scores_rating\n",
    "model1.score(vectors[test], listings_targets[test][review_scores_rating_column])\n",
    "# Model 2: review_scores_accuracy\n",
    "model2.score(vectors[test], listings_targets[test][review_scores_accuracy_column])\n",
    "# Model 3: review_scores_cleanliness\n",
    "model3.score(vectors[test], listings_targets[test][review_scores_cleanliness_column])\n",
    "# Model 4: review_scores_checkin\n",
    "model4.score(vectors[test], listings_targets[test][review_scores_checkin_column])\n",
    "# Model 5: review_scores_communication\n",
    "model5.score(vectors[test], listings_targets[test][review_scores_communication_column])\n",
    "# Model 6: review_scores_location\n",
    "model6.score(vectors[test], listings_targets[test][review_scores_location_column])\n",
    "# Model 7: review_scores_value\n",
    "model7.score(vectors[test], listings_targets[test][review_scores_value_column])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d823b2eefd00db7fedb7fb746dd6ea2e3dd5dc7a474e97205a727c98ade21320"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
